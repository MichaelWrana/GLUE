{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b6b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from statistics import mean\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198d8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collects random samples from trace with id2 and computes the matrix profile of class1 compared with class 2\n",
    "\n",
    "Input: \n",
    "    trace1: packet traces from class 1\n",
    "    id2: id number for class 2 \n",
    "    num_traces: number of traces to select from class 2 (should be equal to class 1)\n",
    "    shapelet_size: length of shapelets\n",
    "    \n",
    "Output:\n",
    "    Matrix profile of trace1 compared with trace2\n",
    "'''\n",
    "def compare_profile(trace1, trace2, shapelet_size):\n",
    "    \n",
    "    length_diff = len(trace2) - len(trace1)\n",
    "    if(length_diff < 0):\n",
    "        trace2 = np.append(trace2, [np.nan] * abs(length_diff))\n",
    "        \n",
    "    #print(len(trace1))\n",
    "    #print(len(trace2))\n",
    "        \n",
    "    \n",
    "    c1_c2 = stumpy.stump(trace1, shapelet_size, trace2, ignore_trivial=False)[:, 0].astype(float)\n",
    "    c1_c2[c1_c2 == np.inf] = np.nan\n",
    "    \n",
    "    return c1_c2\n",
    "\n",
    "'''\n",
    "Compares a the matrix profile of a class trace with itself\n",
    "\n",
    "Input: \n",
    "    trace: packet traces from class 1\n",
    "    shapelet_size: length of shapelets\n",
    "    \n",
    "Output:\n",
    "    Matrix profile of trace compared with trace\n",
    "'''\n",
    "\n",
    "def same_profile(trace, shapelet_size):\n",
    "    \n",
    "    c1_c1 = stumpy.stump(trace, shapelet_size)[:, 0].astype(float)\n",
    "    c1_c1[c1_c1 == np.inf] = np.nan\n",
    "    \n",
    "    return c1_c1\n",
    "\n",
    "'''\n",
    "return indices of shapelet as one-hot encoded list\n",
    "'''\n",
    "def generate_shapelet(trace, diff, shapelet_size):\n",
    "    \n",
    "    idx = np.argmax(diff)\n",
    "    shapelet = np.asarray([1 if idx <= i < idx + shapelet_size else 0 for i in range(len(trace))])\n",
    "    \n",
    "    return shapelet\n",
    "\n",
    "'''\n",
    "Compute shapelet of greatest overlaps\n",
    "'''\n",
    "def find_overlap(trace_i, shapelets_i, shapelet_size):\n",
    "    #print(shapelets_i[0])\n",
    "    \n",
    "    merged_shapelets = np.sum(shapelets_i, axis=0)\n",
    "    \n",
    "    max_size = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "    for i in range(0, len(merged_shapelets), shapelet_size):\n",
    "        current_size = np.sum(merged_shapelets[i:i+shapelet_size])\n",
    "        if current_size > max_size:\n",
    "            max_size = current_size\n",
    "            start = i\n",
    "            end = i + shapelet_size\n",
    "    \n",
    "    return trace_i[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41239be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates a set of 100 shapelets for each class in samples\n",
    "\n",
    "Input:\n",
    "    num_traces = Number of traces per class\n",
    "    shapelet_size = Size of shapelets\n",
    "    save: save results to file?\n",
    "    filename: if save, name & location of output file\n",
    "\n",
    "Output:\n",
    "    list object containing shapelets for each class\n",
    "\n",
    "'''\n",
    "def generate_shapelets(shapelet_coeff):\n",
    "    shapelet_storage = []\n",
    "    \n",
    "    # loop over all classes (generate shapelet for each class)\n",
    "    for i in tqdm(range(100)):\n",
    "        \n",
    "        # get the chosen sample from trace i\n",
    "        trace_i = chosen_traces[i].astype('float64')\n",
    "        shapelet_size = math.floor(shapelet_coeff * len(trace_i))\n",
    "        \n",
    "        shapelets_i = np.zeros((100, len(trace_i)))\n",
    "        #print(shapelets_i.shape)\n",
    "        \n",
    "        # generate profile of i compared with itself\n",
    "        # length of sample is coeff* len*trace_i\n",
    "        ci_ci = same_profile(trace_i, shapelet_size)\n",
    "        \n",
    "        # loop over every other class and generate a profile for each one\n",
    "        for j in range(100):\n",
    "            # don't compare i with itself \n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            trace_j = chosen_traces[j].astype('float64')\n",
    "            \n",
    "            # compute profile of i compared with j\n",
    "            ci_cj = compare_profile(trace_i, trace_j, shapelet_size)\n",
    "\n",
    "            # find largest value gap between other and i\n",
    "            diff_ci = ci_cj - ci_ci\n",
    "            \n",
    "            # generate best shapelet for i compared to j and store it in list\n",
    "            ci_shape = generate_shapelet(trace_i, diff_ci, shapelet_size)\n",
    "            shapelets_i[j] = ci_shape\n",
    "        \n",
    "        # compare shapelets between all classes and return the one which has the most overlap\n",
    "        # (i.e.) the shapelet that was chosen most between the 99 other classes\n",
    "        best_shapelet = find_overlap(trace_i, shapelets_i, shapelet_size)\n",
    "        # save to list\n",
    "        shapelet_storage.append(best_shapelet)\n",
    "    \n",
    "    return shapelet_storage   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651bd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the minimum distance beteen data samples and shapelets\n",
    "Input:\n",
    "    data = list of individual packet traces\n",
    "    shapelets = list of shapelets\n",
    "Output:\n",
    "    minimum distance between each sample in data compared with each sample in shapelet\n",
    "    shape = (len(data),len(shapelets))\n",
    "'''\n",
    "def distance_to_shapelet(data, shapelets):\n",
    "    #data = np.asarray(data)\n",
    "    #print(len(data))\n",
    "    \n",
    "    # processed output data\n",
    "    data_out = np.zeros((len(data),len(shapelets)))\n",
    "    \n",
    "    # loop over each sample in the dataset\n",
    "    for i,sample in enumerate(tqdm(data)):\n",
    "        shapelet_score = np.empty(len(shapelets))\n",
    "        # for each shapelet, calculate distance and assign a score\n",
    "        for j,shapelet in enumerate(shapelets):\n",
    "            try:\n",
    "                dist = stumpy.mass(shapelet, sample)\n",
    "            except ValueError:\n",
    "                dist = stumpy.mass(sample, shapelet)\n",
    "            shapelet_score[j] = dist.min()\n",
    "        data_out[i] = shapelet_score\n",
    "    \n",
    "    return data_out\n",
    "\n",
    "'''\n",
    "Computes distances between input samples and shapelets, returns X for classifier\n",
    "Also cleans data and ensures no random errors due to length, NaN, etc...\n",
    "Underlying function that performs comparison is distance_to_shapelet\n",
    "Selects data samples (with replacement)\n",
    "note: some samples will always be bad so actual length of X is less\n",
    "\n",
    "Input:\n",
    "    num_traces = numner of traces to process\n",
    "    save = save output to file\n",
    "    filenames = tuple that represents (name of X file, name of y file)\n",
    "\n",
    "Output:\n",
    "    X values for classifier of shape (None, 100)\n",
    "    y values for classifier of shape (None, )\n",
    "'''\n",
    "\n",
    "def process_traces(num_traces, shapelets):\n",
    "    X, y = [], []\n",
    "\n",
    "    \n",
    "#     for i in range(num_traces):\n",
    "#         combo_trace = []\n",
    "#         combo_trace.append(random.choice(traces[random.randint(50,99)]))\n",
    "#         y_id = random.randint(0,49)\n",
    "#         combo_trace.append(random.choice(traces[y_id]))\n",
    "#         combo_trace.append(random.choice(traces[random.randint(50,99)]))\n",
    "#         out = np.concatenate((combo_trace[0],combo_trace[1],combo_trace[2]))\n",
    "        \n",
    "#         X.append(out)\n",
    "#         y.append(y_id)\n",
    "\n",
    "    # iterate over dictionary and re-format into X and y\n",
    "    for trace_id, trace_vals in traces.items():\n",
    "        for trace in trace_vals:\n",
    "            X.append(trace)\n",
    "            y.append(trace_id)\n",
    "    \n",
    "#     for i in range(num_traces):\n",
    "#         random_id = random.randrange(100)\n",
    "#         random_trace = random.choice(traces[random_id])\n",
    "#         X.append([random_trace])\n",
    "#         y.append(random_id)\n",
    "    \n",
    "    print(\"Size of X: \" + str(len(X)))\n",
    "    \n",
    "    \n",
    "    # process and remove useless entries (too short)\n",
    "    X = [np.asarray(trace).astype('float64') for trace in X]\n",
    "    X = [trace[~np.isnan(trace)] for trace in X]    \n",
    "\n",
    "    # compute distance between input trace and shapelet arrays\n",
    "    # return as new X\n",
    "\n",
    "    X = distance_to_shapelet(X, shapelets)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "485914fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate performance of sklearn classifier on data samples - 90/10 training testing split\n",
    "\n",
    "Input:\n",
    "    clf: sklearn classifier object\n",
    "    X: x values\n",
    "    y: y values\n",
    "    topk: k values for evaluation metrics\n",
    "Output:\n",
    "    list of length topk with accuracy for testing data\n",
    "'''\n",
    "\n",
    "def classifier_performance(clf, X, y, topk=[1,3,5]):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    \n",
    "    scores = []\n",
    "    for k in topk:\n",
    "        correct = 0\n",
    "        for i in range(len(y_prob)):\n",
    "            ind = np.argpartition(y_prob[i], -k)[-k:]\n",
    "            if y_test[i] in ind:\n",
    "                correct += 1\n",
    "        scores.append(correct/len(y_prob))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146c66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility function for pipeline of evaluating different grid search parameters\n",
    "Output: a new file located in ../results/param1-val1_param2-val2_param3-val3\n",
    "        the file contains a pickled python object\n",
    "        with the scores for top-1, top-3, and top-5 classifier accuracy\n",
    "'''\n",
    "# note: python multiprocessing is really annoying to work with\n",
    "# function needs to be in a separate .py file which is imported\n",
    "# and function can only have 1 argument\n",
    "# list input which is immediately used for what would be the arguments\n",
    "def evaluate_parameters(arr):\n",
    "    \n",
    "    num_experiment = arr[0]\n",
    "    shapelet_coeff = arr[1]\n",
    "    num_samples = 0\n",
    "    \n",
    "    filename = '../results/shapelets/' + 'num=' + str(num_experiment) + 'size=' + str(shapelet_coeff)\n",
    "    #filename = '../results/data/trace_choice'\n",
    "    with open(filename, 'rb') as f:\n",
    "        shapelets = pickle.load(f)\n",
    "    \n",
    "    shapelets = [shapelet.astype('float64') for shapelet in shapelets]\n",
    "    \n",
    "    X, y = process_traces(num_samples, shapelets)\n",
    "    \n",
    "    filename = '../results/data/X/' + 'num=' + str(num_experiment) + 'size=' + str(shapelet_coeff)\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "        \n",
    "    filename = '../results/data/y/' + 'num=' + str(num_experiment) + 'size=' + str(shapelet_coeff)\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8b92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "global traces\n",
    "\n",
    "with open('../ipt_traces.npy', 'rb') as f:\n",
    "    traces = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e5bd4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.25], [1, 0.25], [2, 0.25], [3, 0.25], [4, 0.25], [5, 0.25], [6, 0.25], [7, 0.25], [8, 0.25], [9, 0.25], [10, 0.25], [11, 0.25], [12, 0.25], [13, 0.25]]\n"
     ]
    }
   ],
   "source": [
    "nums = list(range(14))\n",
    "size = [0.25]\n",
    "\n",
    "parameter_list = [[x,y] for x in nums for y in size]\n",
    "\n",
    "print(parameter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac00d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1\n",
    "\n",
    "print(parameter_list)\n",
    "\n",
    "for parameters in parameter_list:\n",
    "    coeff = parameters[1]\n",
    "    shapelets = generate_shapelets(coeff)\n",
    "    \n",
    "    filename = '../results/shapelets/' + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1])\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(shapelets, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060f4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.25], [1, 0.25], [2, 0.25], [3, 0.25], [4, 0.25], [5, 0.25], [6, 0.25], [7, 0.25], [8, 0.25], [9, 0.25], [10, 0.25], [11, 0.25], [12, 0.25], [13, 0.25]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/450000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n",
      "Size of X: 450000\n",
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/450000 [00:00<?, ?it/s]\r",
      "  0%|          | 0/450000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/450000 [00:00<?, ?it/s], 12.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n",
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450000/450000 [5:59:22<00:00, 20.87it/s]s]\n",
      " 75%|███████▍  | 337041/450000 [5:59:50<1:22:29, 22.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450000/450000 [7:31:22<00:00, 16.62it/s]] \n",
      " 21%|██        | 94983/450000 [1:32:06<4:36:56, 21.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450000/450000 [7:34:11<00:00, 16.51it/s]] \n",
      "  1%|          | 2482/450000 [02:49<8:53:37, 13.98it/s]s]] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450000/450000 [7:35:27<00:00, 16.47it/s]]\n",
      "100%|██████████| 450000/450000 [7:35:51<00:00, 16.45it/s]s]\n",
      "  0%|          | 0/450000 [00:00<?, ?it/s]8, 12.52it/s]t/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 416394/450000 [7:36:24<48:27, 11.56it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450000/450000 [8:14:40<00:00, 15.16it/s]s]\n",
      " 10%|▉         | 44780/450000 [39:14<6:52:40, 16.37it/s]/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450000/450000 [7:08:07<00:00, 17.52it/s]s]\n",
      " 79%|███████▉  | 356917/450000 [5:36:36<1:35:49, 16.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450000/450000 [6:41:42<00:00, 18.67it/s]]]\n",
      " 14%|█▎        | 61870/450000 [1:09:46<10:17:52, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450000/450000 [7:04:17<00:00, 17.68it/s]] \n",
      "100%|██████████| 450000/450000 [7:11:36<00:00, 17.38it/s]] \n",
      "100%|██████████| 450000/450000 [7:27:59<00:00, 16.74it/s]s]\n",
      "100%|██████████| 450000/450000 [6:58:24<00:00, 17.93it/s]s]\n",
      " 30%|██▉       | 133788/450000 [2:26:15<4:00:50, 21.88it/s] "
     ]
    }
   ],
   "source": [
    "## PART 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    global traces\n",
    "\n",
    "    with open('../nonzero_traces.npy', 'rb') as f:\n",
    "        traces = pickle.load(f)\n",
    "\n",
    "    from utils import evaluate_parameters\n",
    "    print(parameter_list)\n",
    "    \n",
    "    with Pool(6) as p:\n",
    "        p.map(evaluate_parameters, parameter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad0c0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450000, 1185)\n",
      "(450000,)\n"
     ]
    }
   ],
   "source": [
    "# merge X values from different datasets\n",
    "\n",
    "folder_X = \"../results/data/X/\"\n",
    "\n",
    "X = ()\n",
    "\n",
    "for filename in os.listdir(folder_X):\n",
    "    if filename[0] != '.' :\n",
    "        with open(folder_X + filename, 'rb') as f:\n",
    "            Xi = pickle.load(f)\n",
    "        X = X + (Xi,) \n",
    "        \n",
    "X = np.concatenate(X, axis=1)\n",
    "print(X.shape)\n",
    "\n",
    "with open(\"../results/data/y/num=0size=0.25\", 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "y = np.array(y) \n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9301bbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7481777777777778, 0.8634888888888889, 0.9009555555555555]\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "scores = classifier_performance(clf, X, y)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ffae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405000, 1185)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "057f54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1024),\n",
    "    Dense(512),\n",
    "    Dense(256),\n",
    "    Dense(128),\n",
    "    Dense(100)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0260a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 46.8065 - accuracy: 0.2715\n",
      "Epoch 2/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 25.2025 - accuracy: 0.3891\n",
      "Epoch 3/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 32.2632 - accuracy: 0.4075\n",
      "Epoch 4/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 12.2112 - accuracy: 0.4349\n",
      "Epoch 5/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 32.6115 - accuracy: 0.4816\n",
      "Epoch 6/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 11.8555 - accuracy: 0.5105\n",
      "Epoch 7/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 15.5874 - accuracy: 0.4953\n",
      "Epoch 8/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 12.4468 - accuracy: 0.5303\n",
      "Epoch 9/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 31.0755 - accuracy: 0.4978\n",
      "Epoch 10/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 14.3634 - accuracy: 0.5808\n",
      "Epoch 11/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 16.6106 - accuracy: 0.5210\n",
      "Epoch 12/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 8.0719 - accuracy: 0.5738\n",
      "Epoch 13/1000\n",
      "12657/12657 [==============================] - 139s 11ms/step - loss: 12.6449 - accuracy: 0.5484\n",
      "Epoch 14/1000\n",
      "12657/12657 [==============================] - 1020s 81ms/step - loss: 7.9470 - accuracy: 0.5847\n",
      "Epoch 15/1000\n",
      "12657/12657 [==============================] - 717s 57ms/step - loss: 5.4013 - accuracy: 0.5725\n",
      "Epoch 16/1000\n",
      "12657/12657 [==============================] - 53s 4ms/step - loss: 6.7824 - accuracy: 0.5823\n",
      "Epoch 17/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.0915 - accuracy: 0.5835\n",
      "Epoch 18/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.6281 - accuracy: 0.5947\n",
      "Epoch 19/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 11.5549 - accuracy: 0.5947\n",
      "Epoch 20/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 6.5320 - accuracy: 0.6083\n",
      "Epoch 21/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 12.6973 - accuracy: 0.6038\n",
      "Epoch 22/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 17.7017 - accuracy: 0.6094\n",
      "Epoch 23/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.5312 - accuracy: 0.6135\n",
      "Epoch 24/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.0503 - accuracy: 0.6266\n",
      "Epoch 25/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.3968 - accuracy: 0.6062\n",
      "Epoch 26/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.0487 - accuracy: 0.6350\n",
      "Epoch 27/1000\n",
      "12657/12657 [==============================] - 1348s 107ms/step - loss: 15.0195 - accuracy: 0.6212\n",
      "Epoch 28/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.7755 - accuracy: 0.6374\n",
      "Epoch 29/1000\n",
      "12657/12657 [==============================] - 2030s 160ms/step - loss: 6.5551 - accuracy: 0.6430\n",
      "Epoch 30/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.7669 - accuracy: 0.6466\n",
      "Epoch 31/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.9976 - accuracy: 0.6436\n",
      "Epoch 32/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.1023 - accuracy: 0.6408\n",
      "Epoch 33/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 5.4550 - accuracy: 0.6491\n",
      "Epoch 34/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 4.6471 - accuracy: 0.6398\n",
      "Epoch 35/1000\n",
      "12657/12657 [==============================] - 57s 4ms/step - loss: 11.0384 - accuracy: 0.6470\n",
      "Epoch 36/1000\n",
      "12657/12657 [==============================] - 58s 5ms/step - loss: 4.8862 - accuracy: 0.6625\n",
      "Epoch 37/1000\n",
      "12657/12657 [==============================] - 1074s 85ms/step - loss: 7.5762 - accuracy: 0.6486\n",
      "Epoch 38/1000\n",
      "12657/12657 [==============================] - 882s 70ms/step - loss: 6.5002 - accuracy: 0.6577\n",
      "Epoch 39/1000\n",
      "12657/12657 [==============================] - 1076s 85ms/step - loss: 7.1707 - accuracy: 0.6492\n",
      "Epoch 40/1000\n",
      "12657/12657 [==============================] - 452s 36ms/step - loss: 6.9211 - accuracy: 0.6504\n",
      "Epoch 41/1000\n",
      "12657/12657 [==============================] - 55s 4ms/step - loss: 6.6249 - accuracy: 0.6483\n",
      "Epoch 42/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 14.3050 - accuracy: 0.6440\n",
      "Epoch 43/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 8.5018 - accuracy: 0.6567\n",
      "Epoch 44/1000\n",
      "12657/12657 [==============================] - 53s 4ms/step - loss: 7.8649 - accuracy: 0.6559\n",
      "Epoch 45/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 5.9650 - accuracy: 0.6578\n",
      "Epoch 46/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.1294 - accuracy: 0.6587\n",
      "Epoch 47/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.9122 - accuracy: 0.6641\n",
      "Epoch 48/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 3.7711 - accuracy: 0.6739\n",
      "Epoch 49/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.5809 - accuracy: 0.6626\n",
      "Epoch 50/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.9647 - accuracy: 0.6604\n",
      "Epoch 51/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.1271 - accuracy: 0.6603\n",
      "Epoch 52/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.1295 - accuracy: 0.6801\n",
      "Epoch 53/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.7510 - accuracy: 0.6827\n",
      "Epoch 54/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.9081 - accuracy: 0.6742\n",
      "Epoch 55/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.5483 - accuracy: 0.6688\n",
      "Epoch 56/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 6.2927 - accuracy: 0.6715\n",
      "Epoch 57/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 11.4145 - accuracy: 0.6637\n",
      "Epoch 58/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 5.0340 - accuracy: 0.6845\n",
      "Epoch 59/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 4.9079 - accuracy: 0.6884\n",
      "Epoch 60/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.3848 - accuracy: 0.6810\n",
      "Epoch 61/1000\n",
      "12657/12657 [==============================] - 53s 4ms/step - loss: 5.6753 - accuracy: 0.6748\n",
      "Epoch 62/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 13.4054 - accuracy: 0.6611\n",
      "Epoch 63/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.0492 - accuracy: 0.6760\n",
      "Epoch 64/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 8.5148 - accuracy: 0.6861\n",
      "Epoch 65/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 11.2854 - accuracy: 0.6670\n",
      "Epoch 66/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.0166 - accuracy: 0.6720\n",
      "Epoch 67/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.4797 - accuracy: 0.6845\n",
      "Epoch 68/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.4916 - accuracy: 0.6790\n",
      "Epoch 69/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.9427 - accuracy: 0.6783\n",
      "Epoch 70/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.1874 - accuracy: 0.6732\n",
      "Epoch 71/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.7135 - accuracy: 0.6759\n",
      "Epoch 72/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.3297 - accuracy: 0.6785\n",
      "Epoch 73/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.4749 - accuracy: 0.6920\n",
      "Epoch 74/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.5724 - accuracy: 0.6837\n",
      "Epoch 75/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.3998 - accuracy: 0.6812\n",
      "Epoch 76/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 6.7298 - accuracy: 0.6811\n",
      "Epoch 77/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 14.9626 - accuracy: 0.6763\n",
      "Epoch 78/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 6.1043 - accuracy: 0.6858\n",
      "Epoch 79/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 6.7025 - accuracy: 0.6876\n",
      "Epoch 80/1000\n",
      "12657/12657 [==============================] - 805s 64ms/step - loss: 7.8080 - accuracy: 0.6906\n",
      "Epoch 81/1000\n",
      "12657/12657 [==============================] - 964s 76ms/step - loss: 11.5735 - accuracy: 0.6830\n",
      "Epoch 82/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.5091 - accuracy: 0.6833\n",
      "Epoch 83/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.1909 - accuracy: 0.6831\n",
      "Epoch 84/1000\n",
      "12657/12657 [==============================] - 311s 25ms/step - loss: 7.0679 - accuracy: 0.6962\n",
      "Epoch 85/1000\n",
      "12657/12657 [==============================] - 72s 6ms/step - loss: 6.6299 - accuracy: 0.6797\n",
      "Epoch 86/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.8633 - accuracy: 0.6794\n",
      "Epoch 87/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 13.3424 - accuracy: 0.6852\n",
      "Epoch 88/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 12.3990 - accuracy: 0.6759\n",
      "Epoch 89/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.7964 - accuracy: 0.6956\n",
      "Epoch 90/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.5607 - accuracy: 0.6878\n",
      "Epoch 91/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 3.6443 - accuracy: 0.6977\n",
      "Epoch 92/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.3785 - accuracy: 0.6918\n",
      "Epoch 93/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.8614 - accuracy: 0.6870\n",
      "Epoch 94/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.9009 - accuracy: 0.6963\n",
      "Epoch 95/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 8.6973 - accuracy: 0.6860\n",
      "Epoch 96/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 10.3423 - accuracy: 0.6921\n",
      "Epoch 97/1000\n",
      "12657/12657 [==============================] - 104s 8ms/step - loss: 8.8903 - accuracy: 0.7031\n",
      "Epoch 98/1000\n",
      "12657/12657 [==============================] - 984s 78ms/step - loss: 5.8657 - accuracy: 0.7010\n",
      "Epoch 99/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.5763 - accuracy: 0.6908\n",
      "Epoch 100/1000\n",
      "12657/12657 [==============================] - 62s 5ms/step - loss: 6.8942 - accuracy: 0.6900\n",
      "Epoch 101/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 13.3084 - accuracy: 0.6769\n",
      "Epoch 102/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.5352 - accuracy: 0.7028\n",
      "Epoch 103/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.3535 - accuracy: 0.7028\n",
      "Epoch 104/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.7263 - accuracy: 0.7043\n",
      "Epoch 105/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.5881 - accuracy: 0.6988\n",
      "Epoch 106/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 11.0842 - accuracy: 0.6816\n",
      "Epoch 107/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.6764 - accuracy: 0.6957\n",
      "Epoch 108/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 8.5658 - accuracy: 0.6991\n",
      "Epoch 109/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.5494 - accuracy: 0.7053\n",
      "Epoch 110/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.1933 - accuracy: 0.6954\n",
      "Epoch 111/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.7824 - accuracy: 0.6973\n",
      "Epoch 112/1000\n",
      "12657/12657 [==============================] - 102s 8ms/step - loss: 11.0852 - accuracy: 0.6932\n",
      "Epoch 113/1000\n",
      "12657/12657 [==============================] - 1943s 154ms/step - loss: 9.4538 - accuracy: 0.6886\n",
      "Epoch 114/1000\n",
      "12657/12657 [==============================] - 236s 19ms/step - loss: 4.6617 - accuracy: 0.6953\n",
      "Epoch 115/1000\n",
      "12657/12657 [==============================] - 999s 79ms/step - loss: 8.0181 - accuracy: 0.6926\n",
      "Epoch 116/1000\n",
      "12657/12657 [==============================] - 220s 17ms/step - loss: 15.5017 - accuracy: 0.6854\n",
      "Epoch 117/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 8.6271 - accuracy: 0.6935\n",
      "Epoch 118/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 5.0423 - accuracy: 0.7103\n",
      "Epoch 119/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.9487 - accuracy: 0.7025\n",
      "Epoch 120/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.9094 - accuracy: 0.7019\n",
      "Epoch 121/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.3478 - accuracy: 0.6930\n",
      "Epoch 122/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 8.3355 - accuracy: 0.6912\n",
      "Epoch 123/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 13.4108 - accuracy: 0.6887\n",
      "Epoch 124/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.8300 - accuracy: 0.7103\n",
      "Epoch 125/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 10.3825 - accuracy: 0.7057\n",
      "Epoch 126/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.3347 - accuracy: 0.7032\n",
      "Epoch 127/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.4020 - accuracy: 0.7033\n",
      "Epoch 128/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 10.7998 - accuracy: 0.6924\n",
      "Epoch 129/1000\n",
      "12657/12657 [==============================] - 970s 77ms/step - loss: 5.6097 - accuracy: 0.7025\n",
      "Epoch 130/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.4452 - accuracy: 0.7048\n",
      "Epoch 131/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 6.8563 - accuracy: 0.7053\n",
      "Epoch 132/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 7.8341 - accuracy: 0.7015\n",
      "Epoch 133/1000\n",
      "12657/12657 [==============================] - 56s 4ms/step - loss: 10.4727 - accuracy: 0.6963\n",
      "Epoch 134/1000\n",
      "12657/12657 [==============================] - 59s 5ms/step - loss: 8.3289 - accuracy: 0.6989\n",
      "Epoch 135/1000\n",
      "12657/12657 [==============================] - 61s 5ms/step - loss: 7.0132 - accuracy: 0.6943\n",
      "Epoch 136/1000\n",
      "12657/12657 [==============================] - 65s 5ms/step - loss: 6.4259 - accuracy: 0.7010\n",
      "Epoch 137/1000\n",
      "12657/12657 [==============================] - 63s 5ms/step - loss: 7.5349 - accuracy: 0.6992\n",
      "Epoch 138/1000\n",
      "12657/12657 [==============================] - 1107s 87ms/step - loss: 7.1341 - accuracy: 0.7154\n",
      "Epoch 139/1000\n",
      "12657/12657 [==============================] - 1115s 88ms/step - loss: 5.8954 - accuracy: 0.6987\n",
      "Epoch 140/1000\n",
      "12657/12657 [==============================] - 965s 76ms/step - loss: 5.1948 - accuracy: 0.7054\n",
      "Epoch 141/1000\n",
      "12657/12657 [==============================] - 971s 77ms/step - loss: 8.8021 - accuracy: 0.7042\n",
      "Epoch 142/1000\n",
      "12657/12657 [==============================] - 1020s 81ms/step - loss: 5.1239 - accuracy: 0.6981\n",
      "Epoch 143/1000\n",
      "12657/12657 [==============================] - 980s 77ms/step - loss: 9.9167 - accuracy: 0.6929\n",
      "Epoch 144/1000\n",
      "12657/12657 [==============================] - 965s 76ms/step - loss: 8.4820 - accuracy: 0.7071\n",
      "Epoch 145/1000\n",
      "12657/12657 [==============================] - 1016s 80ms/step - loss: 3.9994 - accuracy: 0.7126\n",
      "Epoch 146/1000\n",
      "12657/12657 [==============================] - 1001s 79ms/step - loss: 5.2471 - accuracy: 0.7089\n",
      "Epoch 147/1000\n",
      "12657/12657 [==============================] - 951s 75ms/step - loss: 12.9995 - accuracy: 0.7018\n",
      "Epoch 148/1000\n",
      "12657/12657 [==============================] - 1049s 83ms/step - loss: 4.5158 - accuracy: 0.7013\n",
      "Epoch 149/1000\n",
      "12657/12657 [==============================] - 1086s 86ms/step - loss: 5.5502 - accuracy: 0.7137\n",
      "Epoch 150/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12657/12657 [==============================] - 970s 77ms/step - loss: 5.8740 - accuracy: 0.7051\n",
      "Epoch 151/1000\n",
      "12657/12657 [==============================] - 1054s 83ms/step - loss: 7.8987 - accuracy: 0.7045\n",
      "Epoch 152/1000\n",
      "12657/12657 [==============================] - 1010s 80ms/step - loss: 7.8774 - accuracy: 0.6965\n",
      "Epoch 153/1000\n",
      "12657/12657 [==============================] - 958s 76ms/step - loss: 5.2089 - accuracy: 0.6990\n",
      "Epoch 154/1000\n",
      "12657/12657 [==============================] - 1022s 81ms/step - loss: 12.3307 - accuracy: 0.6945\n",
      "Epoch 155/1000\n",
      "12657/12657 [==============================] - 1111s 88ms/step - loss: 4.7145 - accuracy: 0.7145\n",
      "Epoch 156/1000\n",
      "12657/12657 [==============================] - 990s 78ms/step - loss: 7.1920 - accuracy: 0.7098\n",
      "Epoch 157/1000\n",
      "12657/12657 [==============================] - 1013s 80ms/step - loss: 6.3448 - accuracy: 0.7104\n",
      "Epoch 158/1000\n",
      "12657/12657 [==============================] - 1024s 81ms/step - loss: 9.5280 - accuracy: 0.7019\n",
      "Epoch 159/1000\n",
      "12657/12657 [==============================] - 968s 76ms/step - loss: 4.1957 - accuracy: 0.7050\n",
      "Epoch 160/1000\n",
      "12657/12657 [==============================] - 121s 10ms/step - loss: 9.1451 - accuracy: 0.6965\n",
      "Epoch 161/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.2538 - accuracy: 0.7149\n",
      "Epoch 162/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 3.6977 - accuracy: 0.7164\n",
      "Epoch 163/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.0135 - accuracy: 0.7134\n",
      "Epoch 164/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.9087 - accuracy: 0.7030\n",
      "Epoch 165/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.9280 - accuracy: 0.7062\n",
      "Epoch 166/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 12.4765 - accuracy: 0.7017\n",
      "Epoch 167/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.7512 - accuracy: 0.7167\n",
      "Epoch 168/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 7.6340 - accuracy: 0.7102\n",
      "Epoch 169/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 7.0413 - accuracy: 0.7123\n",
      "Epoch 170/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 6.2676 - accuracy: 0.7048\n",
      "Epoch 171/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 9.0309 - accuracy: 0.7044\n",
      "Epoch 172/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 7.8780 - accuracy: 0.7235\n",
      "Epoch 173/1000\n",
      "12657/12657 [==============================] - 796s 63ms/step - loss: 9.3354 - accuracy: 0.7037\n",
      "Epoch 174/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 5.1360 - accuracy: 0.7130\n",
      "Epoch 175/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.0088 - accuracy: 0.7114\n",
      "Epoch 176/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 7.3425 - accuracy: 0.7027\n",
      "Epoch 177/1000\n",
      "12657/12657 [==============================] - 55s 4ms/step - loss: 15.5403 - accuracy: 0.6939\n",
      "Epoch 178/1000\n",
      "12657/12657 [==============================] - 53s 4ms/step - loss: 6.5561 - accuracy: 0.7068\n",
      "Epoch 179/1000\n",
      "12657/12657 [==============================] - 55s 4ms/step - loss: 5.7450 - accuracy: 0.7212\n",
      "Epoch 180/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.8944 - accuracy: 0.7210\n",
      "Epoch 181/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 7.3620 - accuracy: 0.7113\n",
      "Epoch 182/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.2331 - accuracy: 0.7168\n",
      "Epoch 183/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 11.9604 - accuracy: 0.7008\n",
      "Epoch 184/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 5.8763 - accuracy: 0.7175\n",
      "Epoch 185/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 4.5988 - accuracy: 0.7163\n",
      "Epoch 186/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.8258 - accuracy: 0.7194\n",
      "Epoch 187/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 4.4791 - accuracy: 0.7184\n",
      "Epoch 188/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 5.2646 - accuracy: 0.7053\n",
      "Epoch 189/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 9.2996 - accuracy: 0.6982\n",
      "Epoch 190/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 7.0130 - accuracy: 0.7077\n",
      "Epoch 191/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 7.1219 - accuracy: 0.7107\n",
      "Epoch 192/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 6.2292 - accuracy: 0.7219\n",
      "Epoch 193/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 9.2966 - accuracy: 0.7100\n",
      "Epoch 194/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 7.0400 - accuracy: 0.7131\n",
      "Epoch 195/1000\n",
      "12657/12657 [==============================] - 54s 4ms/step - loss: 5.4288 - accuracy: 0.7196\n",
      "Epoch 196/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.4723 - accuracy: 0.7123\n",
      "Epoch 197/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 13.9720 - accuracy: 0.6947\n",
      "Epoch 198/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.8352 - accuracy: 0.7215\n",
      "Epoch 199/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 8.5516 - accuracy: 0.7202\n",
      "Epoch 200/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.4222 - accuracy: 0.7231\n",
      "Epoch 201/1000\n",
      "12657/12657 [==============================] - 91s 7ms/step - loss: 7.4210 - accuracy: 0.7092\n",
      "Epoch 202/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.8163 - accuracy: 0.7051\n",
      "Epoch 203/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 14.1096 - accuracy: 0.7068\n",
      "Epoch 204/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 6.2176 - accuracy: 0.7069\n",
      "Epoch 205/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.1333 - accuracy: 0.7263\n",
      "Epoch 206/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.2936 - accuracy: 0.7246\n",
      "Epoch 207/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 4.4702 - accuracy: 0.7136\n",
      "Epoch 208/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.8383 - accuracy: 0.7083\n",
      "Epoch 209/1000\n",
      "12657/12657 [==============================] - 123s 10ms/step - loss: 13.7837 - accuracy: 0.7122\n",
      "Epoch 210/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.2340 - accuracy: 0.7259\n",
      "Epoch 211/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.7387 - accuracy: 0.7312\n",
      "Epoch 212/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.4804 - accuracy: 0.7140\n",
      "Epoch 213/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.4243 - accuracy: 0.7182\n",
      "Epoch 214/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 7.3320 - accuracy: 0.7139\n",
      "Epoch 215/1000\n",
      "12657/12657 [==============================] - 233s 18ms/step - loss: 7.0929 - accuracy: 0.7207\n",
      "Epoch 216/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.0785 - accuracy: 0.7146\n",
      "Epoch 217/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 4.2523 - accuracy: 0.7214\n",
      "Epoch 218/1000\n",
      "12657/12657 [==============================] - 154s 12ms/step - loss: 8.8685 - accuracy: 0.7115\n",
      "Epoch 219/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 8.2852 - accuracy: 0.7090\n",
      "Epoch 220/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 10.9871 - accuracy: 0.7041\n",
      "Epoch 221/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 8.6187 - accuracy: 0.7133\n",
      "Epoch 222/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 6.0866 - accuracy: 0.7117\n",
      "Epoch 223/1000\n",
      "12657/12657 [==============================] - 1158s 91ms/step - loss: 10.6738 - accuracy: 0.7130\n",
      "Epoch 224/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 10.1836 - accuracy: 0.7061\n",
      "Epoch 225/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 8.0837 - accuracy: 0.7161\n",
      "Epoch 226/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 8.8116 - accuracy: 0.7091\n",
      "Epoch 227/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.5107 - accuracy: 0.7168\n",
      "Epoch 228/1000\n",
      "12657/12657 [==============================] - 172s 14ms/step - loss: 6.4695 - accuracy: 0.7090\n",
      "Epoch 229/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 6.1756 - accuracy: 0.7089\n",
      "Epoch 230/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 5.2361 - accuracy: 0.7162\n",
      "Epoch 231/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 8.7045 - accuracy: 0.7172\n",
      "Epoch 232/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 5.7603 - accuracy: 0.7106\n",
      "Epoch 233/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 4.1530 - accuracy: 0.7264\n",
      "Epoch 234/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.5509 - accuracy: 0.7262\n",
      "Epoch 235/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 9.3359 - accuracy: 0.7097\n",
      "Epoch 236/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 6.6370 - accuracy: 0.7214\n",
      "Epoch 237/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 4.5936 - accuracy: 0.7253\n",
      "Epoch 238/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 7.2192 - accuracy: 0.7171\n",
      "Epoch 239/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 6.1710 - accuracy: 0.7144\n",
      "Epoch 240/1000\n",
      "12657/12657 [==============================] - 52s 4ms/step - loss: 16.2162 - accuracy: 0.7002\n",
      "Epoch 241/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 5.6839 - accuracy: 0.7108\n",
      "Epoch 242/1000\n",
      "12657/12657 [==============================] - 50s 4ms/step - loss: 5.0555 - accuracy: 0.7204\n",
      "Epoch 243/1000\n",
      "12657/12657 [==============================] - 51s 4ms/step - loss: 9.4041 - accuracy: 0.7172\n",
      "Epoch 244/1000\n",
      " 9385/12657 [=====================>........] - ETA: 6:21 - loss: 11.9456 - accuracy: 0.7047"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ww/7kn8t7y91gv3dzhg4tzjhq4m0000gn/T/ipykernel_12279/3108087705.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         ):\n\u001b[1;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       (graph_function,\n\u001b[1;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1861\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART 3\n",
    "\n",
    "print(parameter_list)\n",
    "\n",
    "for parameters in parameter_list:\n",
    "    \n",
    "    filename = '../results/data/X/' + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1])\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    \n",
    "    filename = '../results/data/y/' + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1])\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    scores = classifier_performance(clf, X, y)\n",
    "    \n",
    "    print(scores)\n",
    "    \n",
    "    outfile_name = \"../results/scores/\" + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1])\n",
    "    \n",
    "    with open(outfile_name, 'wb') as f:\n",
    "        pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 4\n",
    "\n",
    "print(parameter_list)\n",
    "\n",
    "for parameters in parameter_list:\n",
    "    \n",
    "    filename = '../results/data/X/' + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1])\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "        \n",
    "    filename = '../results/data/y/' + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1])\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    scores = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    outfile_name = \"../results/scores_perclass/\" + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1])\n",
    "    \n",
    "    with open(outfile_name, 'wb') as f:\n",
    "        pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509ac65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
