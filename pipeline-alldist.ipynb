{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b6b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651bd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_shapelet(data, shapelets):  \n",
    "    dist_size = 2000\n",
    "    fill = np.zeros(dist_size)\n",
    "    \n",
    "    # processed output data\n",
    "    data_out = np.zeros((len(data),len(shapelets), dist_size))\n",
    "    \n",
    "    # loop over each sample in the dataset\n",
    "    for i,sample in enumerate(tqdm(data)):\n",
    "\n",
    "        # for each shapelet, calculate distance and assign a score\n",
    "        for j,shapelet in enumerate(shapelets):\n",
    "            try:\n",
    "                dist = stumpy.mass(shapelet, sample)\n",
    "            except ValueError:\n",
    "                dist = stumpy.mass(sample, shapelet)    \n",
    "            \n",
    "            if len(dist) > dist_size:\n",
    "                dist = dist[:dist_size]\n",
    "            else:\n",
    "                dist = np.concatenate((dist[:dist_size], fill[len(dist):]))\n",
    "            \n",
    "            data_out[i,j] = dist\n",
    "    \n",
    "    return data_out\n",
    "\n",
    "def process_traces(shapelets, namestring):\n",
    "    X, y = [], []\n",
    "    \n",
    "    with open(\"../results/data/distances/\" + namestring + \"min=1\", 'rb') as f:\n",
    "        min_dist_traceids = pickle.load(f) \n",
    "    with open(\"../results/data/distances/\" + namestring + \"min=0\", 'rb') as f:\n",
    "        other_traceids = pickle.load(f)\n",
    "    \n",
    "    for i in tqdm(range(10000)):\n",
    "        main_id = random.choice(min_dist_traceids)\n",
    "        other_id_1 = random.choice(other_traceids)\n",
    "        other_id_2 = random.choice(other_traceids)\n",
    "        \n",
    "        combo_trace = np.concatenate((\n",
    "            random.choice(traces[other_id_1]),\n",
    "            random.choice(traces[main_id]),\n",
    "            random.choice(traces[other_id_2]),\n",
    "        ))\n",
    "\n",
    "        X.append(combo_trace)\n",
    "        y.append(main_id)\n",
    "    \n",
    "    \n",
    "    # iterate over dictionary and re-format into X and y\n",
    "#     for trace_id, trace_vals in traces.items():\n",
    "#         for trace in trace_vals:\n",
    "#             X.append(trace)\n",
    "#             y.append(trace_id)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Processing \" + namestring + \"... \" + \"(\" + str(len(X)) + \" traces)\")\n",
    "    \n",
    "    # convert traces into float64 data type\n",
    "    X = [np.asarray(trace).astype('float64') for trace in X]\n",
    "    # clear empty trace values in data\n",
    "    X = [trace[~np.isnan(trace)] for trace in X]    \n",
    "    # compute distance between input trace and shapelet arrays\n",
    "    # return as new X\n",
    "    X = distance_to_shapelet(X, shapelets)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146c66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: python multiprocessing is really annoying to work with\n",
    "# function needs to be in a separate .py file which is imported\n",
    "# and function can only have 1 argument\n",
    "# list input which is immediately used for what would be the arguments\n",
    "def evaluate_parameters(namestring):\n",
    "    \n",
    "    print(namestring)\n",
    "    \n",
    "    files = {\n",
    "        'shapelets': folder_shapelets + namestring,\n",
    "        'X': folder_X + namestring,\n",
    "        'y': folder_y + namestring\n",
    "    }\n",
    "    try:\n",
    "        with open(files['shapelets'], 'rb') as f:\n",
    "            shapelets = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Shapelet File Missing:\" + files['shapelets'] + \", skipping...\")\n",
    "        return\n",
    "    \n",
    "    shapelets = [shapelet.astype('float64') for shapelet in shapelets]\n",
    "    \n",
    "    X, y = process_traces(shapelets, namestring)\n",
    "    \n",
    "    with open(files['X'], 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "        \n",
    "    with open(files['y'], 'wb') as f:\n",
    "        pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ab9e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5ea7aee49848dd97b1ec32056c538a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing num=2... (10000 traces)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6b49f597124bc38f1b86361ac00886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_parameters(\"num=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485914fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_performance(X, y, perclass, cv=False):\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    if cv:\n",
    "        return np.mean(cross_val_score(clf, X, y, cv=5))\n",
    "    if perclass:\n",
    "        matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "        scores = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    else:\n",
    "        scores = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc42c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_namestring_list(namestring_dict):\n",
    "    \n",
    "    name_components = []\n",
    "    for cat in namestring_dict:\n",
    "        values = namestring_dict[cat]\n",
    "        name_components.append([str(cat) + \"=\" + str(value) for value in values])\n",
    "    \n",
    "    namestring_list = [''.join(item) for item in product(*name_components)]\n",
    "    \n",
    "    return namestring_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef364fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_x(namestring_list, verbose=True):\n",
    "    \n",
    "    X = ()\n",
    "\n",
    "    for filename in namestring_list:\n",
    "        if verbose: print(filename)\n",
    "        with open(folder_X + filename, 'rb') as f:\n",
    "            Xi = pickle.load(f)\n",
    "        X = X + (Xi,) \n",
    "\n",
    "    X = np.concatenate(X, axis=1)\n",
    "\n",
    "    with open(folder_y + namestring_list[0], 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    y = np.array(y) \n",
    "    \n",
    "    if verbose: print(X.shape);print(y.shape);\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29be066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_classifier(namestring_list, perclass=False, repeat=1):\n",
    "        \n",
    "    for name in namestring_list:\n",
    "        with open(folder_X + name, 'rb') as f:\n",
    "            X = pickle.load(f)\n",
    "        with open(folder_y + name, 'rb') as f:\n",
    "            y = pickle.load(f)\n",
    "            \n",
    "        all_scores = []\n",
    "        for i in range(repeat):\n",
    "            clf = RandomForestClassifier()\n",
    "            scores = classifier_performance(X, y, perclass, cv=True)\n",
    "            all_scores.append(scores)\n",
    "        \n",
    "        print(name + \": \" + str(all_scores))\n",
    "        with open(folder_scores + name, 'wb') as f:\n",
    "            pickle.dump(all_scores, f)\n",
    "\n",
    "def batch_merged_classifier(listof_namestring_list, perclass=False, repeat=1):\n",
    "    for namestring_list in listof_namestring_list:\n",
    "        X, y = merge_x(namestring_list, False)\n",
    "        \n",
    "        all_scores = []\n",
    "        for i in range(repeat):\n",
    "            clf = RandomForestClassifier()\n",
    "            scores = classifier_performance(X, y, perclass, cv=True)\n",
    "            all_scores.append(scores)\n",
    "        \n",
    "        outfile = '-'.join(namestring_list) + \"_merged\"\n",
    "        print(outfile + \": \" + str(all_scores))\n",
    "        with open(folder_scores + outfile, 'wb') as f:\n",
    "            pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8b92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP GLOBAL VARIABLES\n",
    "# techincally, in jupyter notebook these dont need to be global and everything will run fine\n",
    "# for the the sake of clarity, I have made them global\n",
    "\n",
    "global traces\n",
    "with open(\"../datasets/ds19.npy\", 'rb') as f:\n",
    "    traces = pickle.load(f)\n",
    "\n",
    "global folder_scores\n",
    "folder_scores = \"../results/scores/\"\n",
    "global folder_shapelets\n",
    "folder_shapelets = \"../results/shapelets/\"\n",
    "global folder_X\n",
    "folder_X = \"../results/data/X/\"\n",
    "global folder_y\n",
    "folder_y = \"../results/data/y/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART 2\n",
    "\n",
    "namestring_list = make_namestring_list({'num':[*range(6)]})\n",
    "print(namestring_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from utils import evaluate_parameters\n",
    "    \n",
    "    with Pool(6) as p:\n",
    "        p.map(evaluate_parameters, namestring_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f1550eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num=1: [0.55676, 0.5526000000000001, 0.55296, 0.55616, 0.5531200000000001]\n"
     ]
    }
   ],
   "source": [
    "batch_classifier([\"num=1\"], repeat=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1745c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num=26size=0-num=33size=0_merged: [0.8752000000000001]\n",
      "num=15size=0-num=13size=0-num=28size=0_merged: [0.8764]\n",
      "num=26size=0-num=33size=0-num=30size=0-num=15size=0_merged: [0.8811]\n",
      "num=14size=0-num=22size=0-num=6size=0-num=19size=0-num=33size=0_merged: [0.8787]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,6):\n",
    "    listof_namestring_list = [make_namestring_list({'num':random.sample(range(36), i), 'size':[0]}) for j in range(1)]\n",
    "    batch_merged_classifier(listof_namestring_list, repeat=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1d797ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['n_clusters=2num=0', 'n_clusters=2num=1'], ['n_clusters=3num=0', 'n_clusters=3num=1', 'n_clusters=3num=2'], ['n_clusters=4num=0', 'n_clusters=4num=1', 'n_clusters=4num=2', 'n_clusters=4num=3']]\n",
      "n_clusters=2num=0-n_clusters=2num=1_merged: [0.8872]\n",
      "n_clusters=3num=0-n_clusters=3num=1-n_clusters=3num=2_merged: [0.8913]\n",
      "n_clusters=4num=0-n_clusters=4num=1-n_clusters=4num=2-n_clusters=4num=3_merged: [0.8943999999999999]\n"
     ]
    }
   ],
   "source": [
    "listof_namestring_list = [\n",
    "    make_namestring_list({'n_clusters':'2','num':[*range(2)]}),\n",
    "    make_namestring_list({'n_clusters':'3','num':[*range(3)]}),\n",
    "    make_namestring_list({'n_clusters':'4','num':[*range(4)]}),\n",
    "]\n",
    "\n",
    "print(listof_namestring_list)\n",
    "\n",
    "batch_merged_classifier(listof_namestring_list, repeat=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ad51fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ddfd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, testing, and validation sets\n",
    "#X,y = merge_x(make_namestring_list({'n_clusters':'4','num':[*range(4)]}), False)\n",
    "X, y = merge_x(make_namestring_list({'norm':'1'}), False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1000)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1000)\n",
    "\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dval_reg = xgb.DMatrix(X_val, y_val, enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "277c263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(params):\n",
    "    print(\"Training with: \", end=\"\")\n",
    "    print(params)\n",
    "    \n",
    "    num_boost_round = params['num_boost_round']\n",
    "    del params['num_boost_round']\n",
    "    \n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain_reg,\n",
    "        num_boost_round=num_boost_round,\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(dval_reg)\n",
    "    score = metrics.accuracy_score(y_val, y_pred)\n",
    "    return {'loss': 1 - score, 'status': STATUS_OK,}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63c2ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Hyper-parameter optimization\n",
    "search_space = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": 100,\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"sampling_method\": 'uniform',\n",
    "    \"num_boost_round\": hp.randint(\"num_boost_round\", 100) + 100,\n",
    "    \"eta\": hp.uniform(\"eta\", 0, 1),\n",
    "    \"gamma\": hp.lognormal(\"gamma\", 0, 1),\n",
    "    \"max_depth\": hp.randint(\"max_depth\", 9) + 1,\n",
    "    \"min_child_weight\": hp.lognormal(\"min_child_weight\", 0, 1),\n",
    "    \"max_delta_step\": hp.lognormal(\"max_delta_step\", 0, 1),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "    \"colsample_bylevel\": hp.uniform(\"colsample_bylevel\", 0.5, 1),\n",
    "    \"colsample_bynode\": hp.uniform(\"colsample_bynode\", 0.5, 1),\n",
    "    \"lambda\": hp.lognormal(\"lambda\", 0, 1),\n",
    "    \"alpha\": hp.lognormal(\"alpha\", 0, 1),\n",
    "    \"tree_method\": hp.choice(\"tree_method\", ['auto', 'exact','approx','hist']),\n",
    "    \"grow_policy\": hp.choice(\"grow_policy\", ['depthwise', 'lossguide']),\n",
    "    \"eval_metric\": hp.choice(\"eval_metric\", ['rmse','rmsle','mae','mape','mphe','mlogloss','merror','map']) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(score, search_space, algo=tpe.suggest, max_evals=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6d37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Reshape, InputLayer, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "057f54f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 1996, 32)      832       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 96, 1996, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 249, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 249, 32)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 245, 64)        51264     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 245, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 1, 30, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 30, 64)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1920)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               192100    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,580\n",
      "Trainable params: 244,388\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filters = [32, 64, 128]\n",
    "kernels = [5, 5, 5]\n",
    "pools = [8, 8, 8]\n",
    "dropout = 0.1\n",
    "\n",
    "model = Sequential([\n",
    "    InputLayer(input_shape=(100,2000,1)),\n",
    "    Conv2D(filters[0], kernels[0], activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pools[0]),\n",
    "    Dropout(dropout),\n",
    "    Conv2D(filters[1], kernels[1], activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pools[1]),\n",
    "    Dropout(dropout),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac306818",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_X + 'num=2', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open(folder_y + 'num=2', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "y = np.asarray(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76edff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "\n",
    "X_train[0].shape\n",
    "X_train[0][0].shape\n",
    "\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0260a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "282/282 [==============================] - 519s 2s/step - loss: 5.0716 - accuracy: 0.0232\n",
      "Epoch 2/1000\n",
      "282/282 [==============================] - 518s 2s/step - loss: 4.2782 - accuracy: 0.0258\n",
      "Epoch 3/1000\n",
      "282/282 [==============================] - 518s 2s/step - loss: 4.0990 - accuracy: 0.0273\n",
      "Epoch 4/1000\n",
      "282/282 [==============================] - 520s 2s/step - loss: 4.0159 - accuracy: 0.0299\n",
      "Epoch 5/1000\n",
      "282/282 [==============================] - 525s 2s/step - loss: 3.9380 - accuracy: 0.0368\n",
      "Epoch 6/1000\n",
      "282/282 [==============================] - 524s 2s/step - loss: 3.8886 - accuracy: 0.0430\n",
      "Epoch 7/1000\n",
      "282/282 [==============================] - 521s 2s/step - loss: 3.8598 - accuracy: 0.0397\n",
      "Epoch 8/1000\n",
      "282/282 [==============================] - 520s 2s/step - loss: 3.8363 - accuracy: 0.0460\n",
      "Epoch 9/1000\n",
      "282/282 [==============================] - 519s 2s/step - loss: 3.8363 - accuracy: 0.0434\n",
      "Epoch 10/1000\n",
      "282/282 [==============================] - 519s 2s/step - loss: 3.8225 - accuracy: 0.0463\n",
      "Epoch 11/1000\n",
      "282/282 [==============================] - 519s 2s/step - loss: 3.8007 - accuracy: 0.0532\n",
      "Epoch 12/1000\n",
      "282/282 [==============================] - 519s 2s/step - loss: 3.7888 - accuracy: 0.0547\n",
      "Epoch 13/1000\n",
      "282/282 [==============================] - 519s 2s/step - loss: 3.7690 - accuracy: 0.0568\n",
      "Epoch 14/1000\n",
      "282/282 [==============================] - 518s 2s/step - loss: 3.7633 - accuracy: 0.0556\n",
      "Epoch 15/1000\n",
      "282/282 [==============================] - 519s 2s/step - loss: 3.7514 - accuracy: 0.0631\n",
      "Epoch 16/1000\n",
      "121/282 [===========>..................] - ETA: 4:57 - loss: 3.7125 - accuracy: 0.0640"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ww/7kn8t7y91gv3dzhg4tzjhq4m0000gn/T/ipykernel_83565/2506763566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         ):\n\u001b[1;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       (graph_function,\n\u001b[1;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1861\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6b64985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 12s 360ms/step\n",
      "[[3.1597254e-05 9.5159578e-08 1.4385321e-04 ... 1.2331117e-10\n",
      "  1.5448155e-08 7.4265478e-07]\n",
      " [7.9201891e-07 2.6188986e-06 3.0598298e-02 ... 6.1860228e-09\n",
      "  1.8800288e-08 8.1609960e-07]\n",
      " [4.5685840e-13 8.4074962e-17 7.4164618e-21 ... 5.3726117e-14\n",
      "  1.3119306e-13 5.1383545e-08]\n",
      " ...\n",
      " [2.8086722e-11 2.3903297e-09 5.5739302e-11 ... 6.4042452e-11\n",
      "  5.9976364e-11 2.8067779e-10]\n",
      " [1.3908936e-12 3.4480934e-12 4.9120999e-08 ... 3.1233267e-12\n",
      "  8.5366394e-15 3.4405909e-12]\n",
      " [1.1686200e-12 7.6550977e-10 7.2792709e-11 ... 4.4209175e-10\n",
      "  3.6672072e-11 3.7610456e-11]]\n",
      "0.842\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "predictions = tf.nn.softmax(predictions)\n",
    "\n",
    "score = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    final_pred = np.argmax(pred, 0)\n",
    "    \n",
    "    if final_pred == y_test[i]:\n",
    "        score += 1\n",
    "\n",
    "\n",
    "        \n",
    "print(score/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a8721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
