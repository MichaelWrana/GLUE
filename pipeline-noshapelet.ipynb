{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b6b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651bd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_shapelet(data, shapelets):    \n",
    "    # processed output data\n",
    "    data_out = np.zeros((len(data),len(shapelets)))\n",
    "    \n",
    "    # loop over each sample in the dataset\n",
    "    for i,sample in enumerate(tqdm(data)):\n",
    "        shapelet_score = np.empty(len(shapelets))\n",
    "        # for each shapelet, calculate distance and assign a score\n",
    "        for j,shapelet in enumerate(shapelets):\n",
    "            try:\n",
    "                dist = stumpy.mass(shapelet, sample)\n",
    "            except ValueError:\n",
    "                dist = stumpy.mass(sample, shapelet)\n",
    "            shapelet_score[j] = dist.min()\n",
    "        data_out[i] = shapelet_score\n",
    "    \n",
    "    return data_out\n",
    "\n",
    "def process_traces(shapelets, name):\n",
    "    X, y = [], []\n",
    "\n",
    "    # iterate over dictionary and re-format into X and y\n",
    "    for trace_id, trace_vals in traces.items():\n",
    "        for trace in trace_vals:\n",
    "            X.append(trace)\n",
    "            y.append(trace_id)\n",
    "    \n",
    "    print(\"Processing\" + name + \"... \" + \"(\" + str(len(X)) + \" traces)\")\n",
    "    \n",
    "    # convert traces into float64 data type\n",
    "    X = [np.asarray(trace).astype('float64') for trace in X]\n",
    "    # clear empty trace values in data\n",
    "    X = [trace[~np.isnan(trace)] for trace in X]    \n",
    "    # compute distance between input trace and shapelet arrays\n",
    "    # return as new X\n",
    "    X = distance_to_shapelet(X, shapelets)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146c66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: python multiprocessing is really annoying to work with\n",
    "# function needs to be in a separate .py file which is imported\n",
    "# and function can only have 1 argument\n",
    "# list input which is immediately used for what would be the arguments\n",
    "def evaluate_parameters(namestring):\n",
    "    \n",
    "    print(namestring)\n",
    "    \n",
    "    files = {\n",
    "        'shapelets': folder_shapelets + namestring,\n",
    "        'X': folder_X + namestring,\n",
    "        'y': folder_y + namestring\n",
    "    }\n",
    "    try:\n",
    "        with open(files['shapelets'], 'rb') as f:\n",
    "            shapelets = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Shapelet File Missing:\" + files['shapelets'] + \", skipping...\")\n",
    "        return\n",
    "    \n",
    "    shapelets = [shapelet.astype('float64') for shapelet in shapelets]\n",
    "    \n",
    "    X, y = process_traces(shapelets, files['shapelets'])\n",
    "    \n",
    "    with open(files['X'], 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "        \n",
    "    with open(files['y'], 'wb') as f:\n",
    "        pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "485914fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_performance(X, y, perclass):\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    if perclass:\n",
    "        matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "        scores = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    else:\n",
    "        scores = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc42c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_namestring_list(namestring_dict):\n",
    "    \n",
    "    name_components = []\n",
    "    for cat in namestring_dict:\n",
    "        values = namestring_dict[cat]\n",
    "        name_components.append([str(cat) + \"=\" + str(value) for value in values])\n",
    "    \n",
    "    namestring_list = [''.join(item) for item in product(*name_components)]\n",
    "    \n",
    "    return namestring_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef364fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_x(namestring_list, verbose=True):\n",
    "    \n",
    "    X = ()\n",
    "\n",
    "    for filename in namestring_list:\n",
    "        if verbose: print(filename)\n",
    "        with open(folder_X + filename, 'rb') as f:\n",
    "            Xi = pickle.load(f)\n",
    "        X = X + (Xi,) \n",
    "\n",
    "    X = np.concatenate(X, axis=1)\n",
    "\n",
    "    with open(folder_y + namestring_list[0], 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    y = np.array(y) \n",
    "    \n",
    "    if verbose: print(X.shape);print(y.shape);\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29be066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_classifier(namestring_list, perclass=False, repeat=1):\n",
    "        \n",
    "    for name in namestring_list:\n",
    "        with open(folder_X + name, 'rb') as f:\n",
    "            X = pickle.load(f)\n",
    "        with open(folder_y + name, 'rb') as f:\n",
    "            y = pickle.load(f)\n",
    "            \n",
    "        all_scores = []\n",
    "        for i in range(repeat):\n",
    "            clf = RandomForestClassifier()\n",
    "            scores = classifier_performance(X, y, perclass)\n",
    "            all_scores.append(scores[0])\n",
    "        \n",
    "        print(name + \": \" + str(all_scores))\n",
    "        with open(folder_scores + name, 'wb') as f:\n",
    "            pickle.dump(all_scores, f)\n",
    "\n",
    "def batch_merged_classifier(listof_namestring_list, perclass=False, repeat=1):\n",
    "    for namestring_list in listof_namestring_list:\n",
    "        X, y = merge_x(namestring_list, False)\n",
    "        \n",
    "        all_scores = []\n",
    "        for i in range(repeat):\n",
    "            clf = RandomForestClassifier()\n",
    "            scores = classifier_performance(X, y, perclass)\n",
    "            all_scores.append(scores[0])\n",
    "        \n",
    "        outfile = '-'.join(namestring_list) + \"_merged\"\n",
    "        print(outfile + \": \" + str(all_scores))\n",
    "        with open(folder_scores + outfile, 'wb') as f:\n",
    "            pickle.dump(all_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8b92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "global traces\n",
    "with open(\"../ds19.npy\", 'rb') as f:\n",
    "    traces = pickle.load(f)\n",
    "\n",
    "global folder_scores\n",
    "folder_scores = \"../results/scores/\"\n",
    "global folder_shapelets\n",
    "folder_shapelets = \"../results/shapelets/\"\n",
    "global folder_X\n",
    "folder_X = \"../results/data/X/\"\n",
    "global folder_y\n",
    "folder_y = \"../results/data/y/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0060f4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_clusters=2num=0', 'n_clusters=2num=1', 'n_clusters=2num=2', 'n_clusters=2num=3', 'n_clusters=3num=0', 'n_clusters=3num=1', 'n_clusters=3num=2', 'n_clusters=3num=3', 'n_clusters=4num=0', 'n_clusters=4num=1', 'n_clusters=4num=2', 'n_clusters=4num=3']\n",
      "n_clusters=2num=2\n",
      "Shapelet File Missing:../results/shapelets/n_clusters=2num=2, skipping...\n",
      "n_clusters=3num=2\n",
      "Processing../results/shapelets/n_clusters=3num=2... (10000 traces)\n",
      "n_clusters=2num=0\n",
      "Processing../results/shapelets/n_clusters=2num=0... (10000 traces)\n",
      "n_clusters=2num=1\n",
      "Processing../results/shapelets/n_clusters=2num=1... (10000 traces)\n",
      "n_clusters=2num=3\n",
      "Shapelet File Missing:../results/shapelets/n_clusters=2num=3, skipping...\n",
      "n_clusters=3num=3\n",
      "Shapelet File Missing:../results/shapelets/n_clusters=3num=3, skipping...\n",
      "n_clusters=4num=0\n",
      "Processing../results/shapelets/n_clusters=4num=0... (10000 traces)\n",
      "n_clusters=3num=1\n",
      "Processing../results/shapelets/n_clusters=3num=1... (10000 traces)\n",
      "n_clusters=3num=0\n",
      "Processing../results/shapelets/n_clusters=3num=0... (10000 traces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [48:45<00:00,  3.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters=4num=1\n",
      "Processing../results/shapelets/n_clusters=4num=1... (10000 traces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [48:45<00:00,  3.42it/s]\n",
      "  0%|          | 4/10000 [00:00<27:06,  6.15it/s]/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters=4num=2\n",
      "Processing../results/shapelets/n_clusters=4num=2... (10000 traces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [48:48<00:00,  3.41it/s]\n",
      "100%|█████████▉| 9988/10000 [48:48<00:04,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters=4num=3\n",
      "Processing../results/shapelets/n_clusters=4num=3... (10000 traces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [48:50<00:00,  3.41it/s]\n",
      "100%|██████████| 10000/10000 [48:54<00:00,  3.41it/s]\n",
      "100%|██████████| 10000/10000 [48:59<00:00,  3.40it/s]\n",
      "100%|██████████| 10000/10000 [48:30<00:00,  3.44it/s] \n",
      "100%|██████████| 10000/10000 [48:32<00:00,  3.43it/s]\n",
      "100%|██████████| 10000/10000 [48:34<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "## PART 2\n",
    "\n",
    "namestring_list = make_namestring_list({'n_clusters':[2,3,4], 'num':[*range(4)]})\n",
    "print(namestring_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from utils import evaluate_parameters\n",
    "    \n",
    "    with Pool(6) as p:\n",
    "        p.map(evaluate_parameters, namestring_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d797ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['n_clusters=2num=0', 'n_clusters=2num=1'], ['n_clusters=3num=0', 'n_clusters=3num=1', 'n_clusters=3num=2'], ['n_clusters=4num=0', 'n_clusters=4num=1', 'n_clusters=4num=2', 'n_clusters=4num=3']]\n",
      "n_clusters=2num=0-n_clusters=2num=1_merged: [0.871, 0.895, 0.883, 0.89, 0.905]\n",
      "n_clusters=3num=0-n_clusters=3num=1-n_clusters=3num=2_merged: [0.903, 0.905, 0.897, 0.883, 0.892]\n",
      "n_clusters=4num=0-n_clusters=4num=1-n_clusters=4num=2-n_clusters=4num=3_merged: [0.905, 0.9, 0.899, 0.889, 0.909]\n"
     ]
    }
   ],
   "source": [
    "listof_namestring_list = [\n",
    "    make_namestring_list({'n_clusters':'2','num':[*range(2)]}),\n",
    "    make_namestring_list({'n_clusters':'3','num':[*range(3)]}),\n",
    "    make_namestring_list({'n_clusters':'4','num':[*range(4)]}),\n",
    "]\n",
    "\n",
    "print(listof_namestring_list)\n",
    "\n",
    "batch_merged_classifier(listof_namestring_list, repeat=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1745c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num=32size=0-num=12size=0-num=0size=0_merged: [0.885, 0.884, 0.858, 0.883, 0.892]\n",
      "num=22size=0-num=2size=0-num=16size=0_merged: [0.877, 0.874, 0.884, 0.886, 0.883]\n",
      "num=1size=0-num=15size=0-num=4size=0_merged: [0.888, 0.884, 0.881, 0.884, 0.882]\n",
      "num=32size=0-num=5size=0-num=16size=0_merged: [0.868, 0.888, 0.874, 0.866, 0.875]\n",
      "num=19size=0-num=27size=0-num=24size=0_merged: [0.886, 0.885, 0.854, 0.877, 0.868]\n",
      "num=4size=0-num=11size=0-num=2size=0-num=27size=0_merged: [0.88, 0.872, 0.873, 0.887, 0.888]\n",
      "num=28size=0-num=21size=0-num=20size=0-num=1size=0_merged: [0.882, 0.878, 0.895, 0.892, 0.887]\n",
      "num=23size=0-num=27size=0-num=16size=0-num=12size=0_merged: [0.876, 0.875, 0.881, 0.882, 0.875]\n",
      "num=26size=0-num=4size=0-num=24size=0-num=8size=0_merged: [0.896, 0.887, 0.888, 0.893, 0.876]\n",
      "num=29size=0-num=33size=0-num=4size=0-num=31size=0_merged: [0.882, 0.894, 0.887, 0.891, 0.889]\n",
      "num=10size=0-num=26size=0-num=18size=0-num=7size=0-num=35size=0_merged: [0.906, 0.882, 0.871, 0.885, 0.875]\n",
      "num=11size=0-num=25size=0-num=8size=0-num=14size=0-num=7size=0_merged: [0.891, 0.873, 0.892, 0.867, 0.881]\n",
      "num=3size=0-num=31size=0-num=23size=0-num=16size=0-num=1size=0_merged: [0.892, 0.872, 0.893, 0.885, 0.884]\n",
      "num=22size=0-num=13size=0-num=14size=0-num=12size=0-num=34size=0_merged: [0.877, 0.894, 0.885, 0.889, 0.902]\n",
      "num=5size=0-num=31size=0-num=29size=0-num=28size=0-num=0size=0_merged: [0.882, 0.889, 0.893, 0.888, 0.895]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,6):\n",
    "    listof_namestring_list = [make_namestring_list({'num':random.sample(range(36), i), 'size':[0]}) for j in range(5)]\n",
    "    batch_merged_classifier(listof_namestring_list, repeat=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9301bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create regression matrices\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 100,\n",
    "    \"tree_method\": \"exact\",\n",
    "    \"device\": \"cpu\",\n",
    "}\n",
    "\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain_reg,\n",
    "    num_boost_round=150,\n",
    "    verbose_eval = True\n",
    ")\n",
    "\n",
    "preds = model.predict(dtest_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c8cf584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.855, 0.948, 0.966]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "topk=[1,3,5]\n",
    "\n",
    "for k in topk:\n",
    "    correct = 0\n",
    "    for i in range(len(preds)):\n",
    "        ind = np.argpartition(preds[i], -k)[-k:]\n",
    "        if y_test[i] in ind:\n",
    "            correct += 1\n",
    "    scores.append(correct/len(preds))\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "057f54f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 14:06:55.169391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 14:07:05.389910: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1024),\n",
    "    Dense(512),\n",
    "    Dense(256),\n",
    "    Dense(128),\n",
    "    Dense(100)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b64985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n",
      "0.583\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = tf.nn.softmax(predictions)\n",
    "\n",
    "score = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    final_pred = np.argmax(pred, 0)\n",
    "    \n",
    "    if final_pred == y_test[i]:\n",
    "        score += 1\n",
    "\n",
    "print(score/len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
